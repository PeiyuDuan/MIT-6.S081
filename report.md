# Report

- [Report](#report)
  - [lab0: 环境搭建](#lab0-环境搭建)
    - [安装QEMU等包](#安装qemu等包)
    - [安装xv6](#安装xv6)
    - [更换QEMU版本](#更换qemu版本)
    - [搭建完成](#搭建完成)
  - [lab1: util](#lab1-util)
    - [阅读笔记](#阅读笔记)
      - [1.1 进程与内存](#11-进程与内存)
      - [1.2 I/O和文件描述符](#12-io和文件描述符)
      - [1.3 管道](#13-管道)
      - [1.4 文件系统](#14-文件系统)
    - [题目](#题目)
      - [sleep](#sleep)
      - [pingpong](#pingpong)
      - [primes](#primes)
      - [find](#find)
      - [xargs](#xargs)
  - [lab2: syscall](#lab2-syscall)
    - [阅读笔记](#阅读笔记-1)
      - [xv6进程概述](#xv6进程概述)
      - [系统调用流程](#系统调用流程)
    - [题目](#题目-1)
      - [system call tracing](#system-call-tracing)
      - [sysinfo](#sysinfo)
  - [lab3: pgtbl](#lab3-pgtbl)
    - [阅读笔记](#阅读笔记-2)
      - [3.1 分页硬件](#31-分页硬件)
      - [3.2 内核地址空间](#32-内核地址空间)
      - [3.3 代码: 创建一个地址空间](#33-代码-创建一个地址空间)
      - [3.5 代码: 物理内存分配](#35-代码-物理内存分配)
      - [剩余部分](#剩余部分)
    - [题目](#题目-2)
      - [Print a page table](#print-a-page-table)
      - [A kernel page table per process](#a-kernel-page-table-per-process)
      - [Simplify copyin/copyinstr](#simplify-copyincopyinstr)
  - [lab4: traps](#lab4-traps)
    - [阅读笔记](#阅读笔记-3)
      - [4.1 RISC-V陷入机制](#41-risc-v陷入机制)
      - [4.2 从用户空间陷入](#42-从用户空间陷入)
      - [4.5 从内核空间陷入](#45-从内核空间陷入)
      - [具体陷入流程](#具体陷入流程)
    - [题目](#题目-3)
      - [RISC-V Assembly](#risc-v-assembly)
      - [Backtrace](#backtrace)
      - [Alarm](#alarm)
  - [lab5: lazy](#lab5-lazy)
    - [阅读笔记](#阅读笔记-4)
    - [题目](#题目-4)
  - [lab6: cow](#lab6-cow)
    - [阅读笔记](#阅读笔记-5)
    - [题目](#题目-5)
  - [lab7: thread](#lab7-thread)
    - [阅读笔记](#阅读笔记-6)
      - [7.1 多路复用](#71-多路复用)
      - [7.2 代码: 上下文切换](#72-代码-上下文切换)
      - [7.3 代码: 调度](#73-代码-调度)
      - [7.4 代码: mycpu和myproc](#74-代码-mycpu和myproc)
      - [总结: 上下文切换流程](#总结-上下文切换流程)
    - [题目](#题目-6)
      - [Uthread: switching between threads](#uthread-switching-between-threads)
      - [using threads](#using-threads)
      - [Barrier](#barrier)
  - [lab8: lock](#lab8-lock)
    - [阅读笔记](#阅读笔记-7)
      - [6.3 代码: 使用锁](#63-代码-使用锁)
      - [6.5 锁和中断处理函数](#65-锁和中断处理函数)
      - [6.9 睡眠锁](#69-睡眠锁)
      - [8.2 Buffer cache层](#82-buffer-cache层)
      - [8.3 代码: Buffer cache](#83-代码-buffer-cache)
    - [题目](#题目-7)
      - [Memory allocator](#memory-allocator)
      - [Buffer cache](#buffer-cache)
  - [lab9: fs](#lab9-fs)
    - [阅读笔记](#阅读笔记-8)
      - [8.4 日志层](#84-日志层)
      - [8.5 日志设计](#85-日志设计)
      - [8.6 代码: 日志](#86-代码-日志)
      - [8.7 代码: 块分配器](#87-代码-块分配器)
      - [8.8 索引节点层](#88-索引节点层)
      - [8.10 代码: Inode包含内容](#810-代码-inode包含内容)
      - [8.12 代码: 路径名](#812-代码-路径名)
      - [8.13 文件描述符层](#813-文件描述符层)
    - [题目](#题目-8)
      - [Large files](#large-files)
      - [Symbolic links](#symbolic-links)
  - [lab10: mmap](#lab10-mmap)
    - [题目](#题目-9)
  - [lab11: net](#lab11-net)
    - [阅读笔记](#阅读笔记-9)
    - [题目](#题目-10)

## lab0: 环境搭建

这个实验在ubuntu22.04上完成，由于与实验中要求20.04不同，有些许步骤不一样。在环境搭建的过程中，需要按这个报告中的搭建。

### 安装QEMU等包

```sh
sudo apt-get install git build-essential gdb-multiarch qemu-system-misc gcc-riscv64-linux-gnu binutils-riscv64-linux-gnu
```

### 安装xv6

```sh
git clone git://g.csail.mit.edu/xv6-labs-2020
```

### 更换QEMU版本

```sh
sudo apt-get remove qemu-system-misc
```

```sh
wget https://download.qemu.org/qemu-5.1.0.tar.xz
tar xf qemu-5.1.0.tar.xz
cd qemu-5.1.0
./configure --disable-kvm --disable-werror --prefix=/usr --target-list="riscv64-softmmu"
make 
sudo make install 
```

此处与实验指导中不同，因为ubuntu22.04已经不支持如下命令，因此需要手动编译

```sh
sudo apt-get install qemu-system-misc=1:4.2-3ubuntu6
```

### 搭建完成

进入xv6目录中，执行`make qemu`即可。

## lab1: util

### 阅读笔记

#### 1.1 进程与内存

1. fork
创建一个所有内容和父进程相同的子进程，但返回值不一样(父进程返回子进程的PID，子进程返回0)。
2. exit
传入0表示成功，1表示失败。
3. wait
当一个子进程终止时，父进程若未终止，则将自己的state设置为ZOMBIE。wait的系统调用会查找所有的ZOMBIE的进程，收回其资源。如果子进程没有退出，则等待。若没有子进程，则返回-1。传入一个地址，接收退出状态。
4. exec
从文件系统加载可执行文件进入进程并执行，该命令后面的指令在exec正确执行的情况下将不会执行。

#### 1.2 I/O和文件描述符

0: 标准输入
1: 标准输出
2: 标准错误

1. read(fd, buf, n)
从文件描述符fd读取最多n字节，将其复制到buf,返回读取的字节数。
2. write(fd, buf, n)
将buf中的n个字节写入文件描述符，并返回读取的字节数。只有发生错误时才会写入小于n字节。
3. close
释放文件描述符，父进程与子进程共享基础文件偏移量

    ```c
    if (fork() == 0) {
      write(1, "hello ", 6);
      exit(0);
    } else {
      wait(0);
      write(1, "world\n", 6);
    }
    ```

    ```text
    hello world
    ```

4. dup
复制一个现有的文件描述符，共享偏移量(与上方fork类似)

#### 1.3 管道

1. pipe一个int[2]的数组用以创建管道
2. 使用read和write对管道进行读写
3. 管道的read如果没有可用数据会一直等待，直到有新数据写入或写入端文件描述符关闭。

#### 1.4 文件系统

1. mkdir 创建新目录
2. open 中若使用 O_CREATE 创建新的数据文件
3. mknod 创建新的设备文件(一个主设备号和一个次设备号唯一标识一个内核设备)
4. 一个文件可以link到多个名字，文件底层是inode保存的。如果两个名字link到同一个文件，则其中的ino(指向inode的编号)相同。

### 题目

#### sleep

**实验目的：**  
了解如何通过系统调用实现进程的暂停，并掌握使用参数传递来控制程序行为的基本方法。

**实验步骤：**  

1. 打开`user/sleep.c`文件。
2. 使用`argc`和`argv`获取命令行参数。
3. 调用`sleep`函数使进程暂停指定时间。
4. 编译并运行代码，验证功能。

**实验中遇到的问题和解决方法：**  
实验过程中没有遇到重大问题。

**实验心得：**  
通过实现sleep命令，进一步理解了系统调用的工作方式以及进程如何通过参数控制行为。

#### pingpong

主要修改 user/prime.c

在此处有可能出现死锁问题(见上方read的等待)。

解决方案: 尽早关闭不需要的描述符。

**实验目的：**  
通过实现一个简单的进程间通信，理解管道的工作机制以及进程同步的基本原理。

**实验步骤：**  

1. 打开并编辑`user/prime.c`文件。
2. 创建父子进程，通过管道实现进程间的消息传递。
3. 编写代码让两个进程互相发送和接收一个字节信息，完成后打印相关内容。
4. 确保及时关闭不需要的文件描述符。

**实验中遇到的问题和解决方法：**  
实验中出现了可能的死锁问题，原因是在读管道时没有数据会阻塞。通过确保在数据传递完毕后关闭不需要的文件描述符，解决了该问题。

**实验心得：**  
通过该实验，理解了管道在进程间通信中的重要作用，同时认识到资源管理在系统编程中的关键性。

#### primes

主要使用了埃氏筛的思想。第一个传入的数字是2，然后把2的倍数全部筛掉，之后3是素数，把3的倍数全部筛掉……到最后可以获取全部的素数。

![显示错误](./Resource/Lab1/sieve.gif)

在此处要求使用进程间通信的方式实现，伪代码如下:

```text
p = get a number from left neighbor
print p
loop:
    n = get a number from left neighbor
    if (p does not divide n)
        send n to right neighbor
```

**实验目的：**  
学习如何通过进程间通信实现并行算法，并掌握经典的埃拉托斯特尼筛法在多进程环境中的实现。

**实验步骤：**  

1. 打开并编辑`primes.c`文件。
2. 设计一个递归函数，创建一系列的进程链来过滤素数。
3. 每个进程接收上一个进程传递的数字，筛选后传递给下一个进程。
4. 使用管道实现进程间的通信，并在处理完数据后及时关闭管道。

**实验中遇到的问题和解决方法：**  
在实现递归进程时，如何高效地管理管道和进程是一个挑战。通过分析数据流和进程生命周期，最终通过优化递归函数逻辑和资源管理解决了这些问题。

**实验心得：**  
实验帮助我理解了并行计算的基本概念以及如何使用进程链来处理复杂的筛选任务。

#### find

**实验目的：**  
理解文件系统中的遍历操作，并实现一个基本的文件查找工具。

**实验步骤：**  

1. 打开并编辑`user/find.c`文件。
2. 参考`ls.c`的实现逻辑，编写代码遍历指定目录。
3. 实现根据文件名或路径匹配查找的功能。
4. 编译并测试代码，确保查找功能正常。

**实验中遇到的问题和解决方法：**  
实验过程相对顺利，只需处理路径和字符串匹配的边界情况即可。

**实验心得：**  
通过编写`find`命令，进一步熟悉了文件系统的基本操作及其在UNIX系统中的实现方式。

#### xargs

该题只需了解清楚xargs的作用，了解清楚题意，则剩下工作较为简单。

了解清楚题意后，写代码更多的是调试指针方面的操作。

- 题目解读
  - 命令行参数
  
    ```text
    mkdir a b c
    a b c 即为参数
    ```

  - 标准化输出

    ```text
    grap a

    匹配输入的字符中是否有a
    命令输出的内容即为标准化输出

    例如，输入
    grap a
    abc
    则标准化输出为 abc
    ```

  - 管道符
  
    ```text
    cmda | cmdb
    将cmda中的输出作为cmdb的输入

    例如 find go | grep demo
    寻找go文件夹中所有包含demo的文件
    ```

  - xargs
  
    ```text
    cmda | xargs cmdb
    cmda的输出会作为cmdb的命令行参数

    例如 echo hello | xargs echo hi
    输出为 hi hello
    ```

**实验目的：**  
理解如何将命令的输出作为另一个命令的参数，并实现一个简单的命令行参数处理工具。

**实验步骤：**  

1. 打开并编辑`user/xargs.c`文件。
2. 读取前一个命令的标准输出，并将其作为新的命令行参数。
3. 处理和拼接命令行参数，执行新的命令。
4. 测试实现的`xargs`工具，确保其与题目要求一致。

**实验中遇到的问题和解决方法：**  
在处理命令行参数时，需要特别注意指针操作和字符串拼接。通过调试和分析，最终解决了这些问题。

**实验心得：**  
`xargs`的实现让我对命令行工具的设计有了更深的理解，尤其是在参数处理和进程管理方面。

## lab2: syscall

### 阅读笔记

这部分要求阅读的内容都非常理论性，因此在此处只标明以前课程中记忆不深刻的，并且在此处说明系统调用的全过程。

#### xv6进程概述

1. 使用页表将虚拟地址映射为物理地址。每个进程都有一个独立的页表，定义了该进程的地址空间。
![显示错误](./Resource/Lab2/va.png)
如图所示，用户地址空间从0开始，最大地址为MAXVA，在地址空间的顶部，有一个trampoline和trapframe分别占用了一页。
2. 每个进程有一个用户栈区和一个内核栈区，当进程在用户指令时，内核栈为空，当执行内核指令时，用户栈保留数据，但不在活跃状态。
3. 使用argint等内容从用户态获取对应的参数。
4. 用户态和内核态的指针不一样，需要使用copyout等方法，找到用户态指针对应的真正的物理地址。

#### 系统调用流程

1. 若在user.h中声明了一个函数，则可以调用它(在别处定义)。但是这样就必须在用户态中执行，因此，不在其中定义，而是想办法在usys.S中调用ecall指令，来到内核态。
2. 根据在usys.S中调用ecall,来到内核态
3. 所有系统函数在syscall()中处理，这个函数根据跳板传入的系统调用编号，找到对应的内核函数的位置。
4. 到具体的位置去执行对应的函数。

注意，usys.S由usys.pl生成。

### 题目

#### system call tracing

**实验目的：**  
通过实现系统调用跟踪功能，了解系统调用的工作原理以及如何在内核中调试和监控进程行为。

**实验步骤：**  

1. 修改`syscall()`函数，添加打印相关系统调用信息的功能。
2. 在`proc.h`中为进程结构体添加一个用于存储跟踪掩码的字段。
3. 修改`fork(proc.c)`函数，确保子进程继承父进程的跟踪掩码。
4. 编译并运行系统，测试系统调用跟踪功能。

**实验中遇到的问题和解决方法：**  
实验中需要确保系统调用跟踪信息的打印不会影响正常的系统行为。通过谨慎处理系统调用输出和进程管理，避免了不必要的系统开销。

**实验心得：**  
通过实现系统调用跟踪功能，深入理解了系统调用的执行流程和内核与用户空间的交互机制，同时掌握了在内核中调试和监控进程的技巧。

#### sysinfo

**实验目的：**  
实现一个系统信息查询功能，了解内核中内存和进程管理的基本原理。

**实验步骤：**  

1. 在`kalloc.c`中实现`get_freememory()`函数，用于获取空闲内存的数量。
2. 在`proc.c`中实现`get_nproc()`函数，用于获取当前进程的数量。
3. 在内核中实现一个新的系统调用，调用上述两个函数并返回系统信息。
4. 编译并测试系统，验证系统信息查询功能。

**实验中遇到的问题和解决方法：**  

在实现空闲内存查询时，需要理解空闲内存页链表的结构。通过仔细分析内核代码，成功遍历链表并计算空闲页数。在进程数量查询中，需要遍历进程表并过滤出有效的进程，最终通过遍历并计数解决了这个问题。

1. 空闲内存页本身直接用作链表节点，每次分配的时候，把链表根部的页分配，回收时把这个页面作为根节点。因此，只需要遍历链表并计数即可。
2. 进程有一个proc table管理(类似于进程池)，里面有使用了的和未使用的进程。因此，只需要遍历这个进程池，找出所有使用了的进程即可。

**实验心得：**  
通过实现`sysinfo`功能，我更深入地理解了内核中内存管理和进程管理的工作原理，也掌握了如何在内核中添加新的系统调用。

## lab3: pgtbl

这个实验比较困难，目前没有完美地做完，因为未知的原因，有的时候可以过，有的时候过不了(在usertests处)。并且，由于内核调试比较困难，参考了许多网上的代码，但是问题并没有解决。并且一直使用的print的方式由于异步而效果不好，而本人又对gdb不是非常熟悉，因此只能暂时搁置。下面附上一张通过的图片。

![显示错误](./Resource/Lab3/succeeded.png)

### 阅读笔记

#### 3.1 分页硬件

1. 页表硬件将虚拟地址映射为物理地址。
2. xv6只使用64位虚拟地址的低39位，页表逻辑上有2^27个页表条目(PTE)。每个PTE包含一个44位的物理页码(PPN)和一些标志。页表的逻辑示图如下。由图可知，页面大小为2^12字节，物理地址有56位。
  ![显示错误](./Resource/Lab3/p1.png)
3. 页表实际的转换分三个步骤进行。页表实际的结构是一个三级的树。树根是一页(4096byte)，其中包含512个PTE(8byte)。每个PTE包含该树的下一级页表页的物理地址。实际的过程如下图所示。
  ![显示错误](./Resource/Lab3/p2.png)
4. 根页表页的物理地址在satp寄存器中。每个cpu都有一个satp。

#### 3.2 内核地址空间

1. xv6为每个进程维护一个页表，内核公用一个页表。
2. 绝大多数虚拟地址都采用直接映射的方式，除了蹦床页面(trampoline)和内核栈页面(保护栈)

#### 3.3 代码: 创建一个地址空间

1. walk
为虚拟地址找到PTE
2. mappages
为新映射装载PTE
3. copyout/copyin
复制数据到用户虚拟地址/从用户虚拟地址复制数据
4. inithart相关
将根页表页的地址写入satp

#### 3.5 代码: 物理内存分配

1. 分配器的数据结构是一个物理内存页的空闲页表。
2. 地址有时以指针存在，有时以uint64存在。
3. kfree将内存的每个字节设置为1。
4. kalloc分配一个页面

#### 剩余部分

1. sbrk
进程减少或增长内存。
2. exec
使用一个存储在文件系统中的文件初始化地址空间的用户部分。使用proc_pagetable分配新页表，使用uvmalloc分配内存，使用loadseg将内容读进来。

### 题目

#### Print a page table

**实验目的：**  
实现并打印页表结构，深入理解分页机制及其在进程地址空间中的应用，并便于以后的调试。

**实验步骤：**  

1. 在`vm.c`中添加`vmprint_aux`和`vmprint`函数。
2. `vmprint`函数打印页表的基本信息，并调用辅助函数。
3. `vmprint_aux`函数通过深度优先搜索打印整个页表。

**实验中遇到的问题和解决方法：**  
在打印页表时，确保递归遍历所有层次，不遗漏任何页表项。通过调试输出验证页表结构的正确性。

**实验心得：**  
通过实现页表打印功能，进一步理解了多级页表结构及其在虚拟地址到物理地址映射中的关键作用。

#### A kernel page table per process

**实验目的：**  
这个题目要求每个进程进入内核态后，都有自己独立的内核页表，为下一个实验做准备。

本题目应该是正确完成了(多次测试usertests均正确)

**实验步骤：**

1. 首先修改struct proc，添加内核页表项 kernelpgtbl (proc.h)
2. 修改kvminit以及相关的函数，并消除相关的影响
    1. 内核中有一些固定的映射，因此将其抽象出函数 kvm_map_pagetable(pagetable_t)。(vm.c)
    2. 抽象出函数kvminit_newpgtbl，返回一个正确的页表。kvminit函数只需要调用这个函数即可。(vm.c)
    3. 修改kvmmap和kvmpa函数，添加第一个参数pagetable_t,便于之后的操作。(vm.c)
    4. 在virtio_disk_rw函数中，调用了kvmpa函数，且与本实验无关，进行处理即可。(virtio_disk.c)
3. 经过以上的操作，可以分配内核页表了。但是，在同一时间内，可能有多个进程同时处于内核态。在原先的设计中，公用一个内核页表，于是需要为不同进程创建多个内核栈，然后map到不同的地方。(见下图Kstack0和Kstack1)
  ![显示错误](./Resource/Lab3/p3.png)
  在修改之后，每个进程都有自己的内核页表，因此只需要map出自己的内核栈即可。因此进行如下操作：
      1. 修改procinit函数，删除创建内核栈的代码。(proc.c)
      2. 在allocproc函数的时候，除了创建kernelpgtbl之外，把内核栈映射到一个确定的位置。(proc.c)
4. 经过以上操作，我们现在可以正确创建页表了。在之后，我们还需要在进程调度时，切换页表。(proc.c/scheduler)
5. 在进程结束后，释放页表(proc.c/freeproc)。在此处，绝对不能使用proc_freepagetable函数(会连带物理页一块释放)和kfree函数(只释放一级页表，造成泄漏)。因此，添加函数kvm_free_kernelpgtbl函数(vm.c)，通过递归的方式释放页表。

**实验中遇到的问题和解决方法：**  
最开始没有意识到内核栈和释放时的问题，通过不断的调试和查阅资料解决。

**实验心得：**  
实现独立的内核页表使我更深刻地理解了页表在保护内核地址空间中的重要作用。

#### Simplify copyin/copyinstr

**实验目的：**  
通过添加独立的内核页表允许直接解引用用户指针。

在这个实验做完后，有时无法通过usertests测试。因此在此只给出自己实现的步骤和理由，但是此部分不保证正确。

**实验步骤：**

1. 两个工具函数:
    1. kvmcopymappings(vm.c)
    将src页表的一部分映射关系(从start到end)拷贝到dst页表中，不拷贝实际的物理内存。成功返回0，不成功返回-1
    2. kvmdealloc(vm.c)
    将程序内存从oldsz缩减为newsz,不释放实际内存。
2. 要解决一个问题。实验中要求，映射的内存地址为0到PLIC，但是这两个之间有一个CLINT的映射。这就产生了冲突。查阅多方资料可知，CLINT仅在内核启动的时候需要使用，因此kvm_map_pagetable()函数中，去除CLINT的映射，在全局页表的初始化(kvminit)中，单独映射。
3. 在exec中检查，避免超过PLIC(exec.c)
4. 在fork(), exec(), growproc()函数中，只要修改了页表，就拷贝到内核页表里面
5. 最后，在userinit(proc.c)函数中，也添加同步映射的代码。

**实验中遇到的问题和解决方法：**  
在处理CLINT映射冲突时，通过分离全局和局部页表映射解决了问题，确保了映射操作的正确性。

**实验心得：**  
通过简化内存复制函数，提高了系统调用的稳定性和内存管理的效率，对页表的灵活性有了更深的理解。

## lab4: traps

### 阅读笔记

#### 4.1 RISC-V陷入机制

一组处理陷阱的寄存器，只能在内核模式下读取或写入。

1. stvec: 保存内核处理陷阱程序的地址
2. sepc: 发生陷阱时，将pc复制到这里。返回时，将这里复制到pc
3. scause: 保存陷阱原因
4. sscratch: 在之后说明
5. sstatus: SIE位控制中断是否启用。SPP位指示陷阱来自用户还是内核。

#### 4.2 从用户空间陷入

1. 几种情况: 用户发出系统调用(ecall)、做了非法的事情、设备中断。
2. 在陷阱的时候不会切换页表，因此必须在用户页表包含uservec(stvec指向的指令)，并且uservec中必须切换satp以切换到内核页表，uservec必须在内核页表和用户页表映射到相同的地址。
3. 以上约束通过trampoline page来满足。映射到的相同地址为 TRAMPOLINE
4. sscratch寄存器的作用是帮助uservec修改一些寄存器的值。在最开始的时候，交换寄存器a0和sscratch的值，uservec可以使用a0寄存器，此时a0中包含了之前sscratch的值。而之前sscratch的值为指向一个陷阱帧的指针，这个帧里面保存了所有用户寄存器。所以此时，uservec就可以在其中保存现场了。

以上内容大致讲述了一些操作中容易迷惑的原理，至于具体的陷入的流程，为了更加清晰，将在下面单独开一个部分来阐述。

#### 4.5 从内核空间陷入

1. 几种情况: 设备中断、异常。
2. 如果由于计时器中断并且一个进程的内核线程正在运行，那么会调用yield来进行调度。

#### 具体陷入流程

以系统调用为例，其余类似。

1. ecall指令(或者导致陷入陷阱的操作)
    ![显示错误](./Resource/Lab4/trap-op.png)
2. uservec函数(trampline.S)
    1. 保存现场
    2. 切换页表、kernel stack
    3. 跳转到usertrap
3. usertrap函数(trap.c)
    1. 修改stvec的值到kerneltrap,再有trap会到那里执行。
    2. 保存sepc的值
    3. 分情况处理(在此处处理系统调用)
4. usertrapret函数(trap.c)
    1. 恢复stvec和sepc的值
    2. 更新trapframe中的值，以便下次的陷入使用
5. userret函数(trampoline.S)
    1. 恢复现场
    2. 把trapframe的内容保存到sscratch中，恢复到用户的pagetable和stack
    3. 跳转到sret
6. sret指令
    1. 切换回user mode
    2. SEPC的寄存器拷贝到PC寄存器
    3. 打开中断

![显示错误](./Resource/Lab4/seq.png)

### 题目

#### RISC-V Assembly

**实验目的：**  
通过理解RISC-V汇编代码，掌握阅读RISC-V代码的能力。

**实验步骤：**  

1. 执行`make fs.img`编译`user/call.c`
2. 阅读call.asm中的代码，回答问题。

#### Backtrace

**实验目的：**  
实现函数调用栈的回溯功能，以便调试和诊断系统问题。

**实验步骤：**  

1. 按照提示，在riscv.h中添加读取s0寄存器的函数，在printf.c中添加backtrace()的函数，并在sys_sleep中调用。之后解读backtrace的实现。
2. backtrace的目的是打印函数调用栈的地址。需要参考如下栈的布局:
  ![显示错误](./Resource/Lab4/p2.png)
  由图可知，栈的返回地址记录在-8到0的位置(首地址为-8)，类似的，栈里面有一个指针指向上一个调用栈，首地址为-16。因此，在循环中，只需要读到当前的栈，然后循环中不断打印返回地址，走到上一个栈即可。根据提示，循环终止条件由页面的顶部地址和底部地址决定。

**实验中遇到的问题和解决方法：**  
无

**实验心得：**  
实现backtrace功能使我更直观地理解了栈帧的组织方式和调试复杂系统时的关键步骤。

#### Alarm

**实验目的：**  
实现定时中断处理，通过sigalarm机制为用户进程添加周期性执行功能。按照题意，添加的sigalarm函数，每次经过interval的时间，就调用handler。

**实验步骤：**  

1. 在proc中添加项目，ticks表示等待的时间，tick_count表示已经经过的时间，handler记录函数，old_trapframe用于恢复原状。(proc.h)
2. 在sys_sigalarm中实现这些变量的修改。注意这里需要重置之前记录的时间(修改后过去的时间应该无效)。(sysproc.c)
3. 在allocproc()函数中实现初始化，在freeproc()函数添加逻辑以释放。(proc.c)
4. 在usertrap中对于时钟周期的中断添加逻辑。(将原先的内容保存，将需要执行的内容写入epc中，这个东西一会会在返回的时候写到pc中，然后会从这个地方继续执行。)
5. 在sys_sigreturn()函数中实现返回的逻辑(sysproc.c)，需要将保存的trapframe存回去，之后就可以在返回的时候执行原来的逻辑。

**实验中遇到的问题和解决方法：**  
防止重复进入信号处理程序，通过tick_count逻辑控制多次进入。将重置tick_count的代码写在sys_sigreturn()里面。也就是说，如果不执行sys_sigreturn()，那么tick_count会一直增加，永远不会满足tick_count == ticks的逻辑，从而不会重新进入。

**实验心得：**  
实现sigalarm机制后，掌握了通过trap机制为用户进程添加额外功能的方法，也学会了在内核中安全处理定时任务。

## lab5: lazy

### 阅读笔记

这部分要阅读的内容比较偏理论，并且比较少，因此我在此部分书写这个实验的原理。下面的lab6同理。

1. 原有逻辑
  ![显示错误](./Resource/Lab5/bef.png)
2. lazy allocation
  ![显示错误](./Resource/Lab5/aft.png)

### 题目

这个实验的前后题目间关联性较强，因此当作一个题目处理，一起写实现的逻辑。

**实验目的：**  
本实验的目的是通过实现“懒分配”（Lazy Allocation），延迟进程内存分配的时机，直到真正需要时才进行实际分配。这种方法优化了内存使用，避免了不必要的内存分配，提升了系统的资源利用率。

**实验步骤：**  

1. 首先，修改sys_sbrk()函数(sysproc.c)，去除增长内存的逻辑growproc，只将大小增大。(如果是内存减小，则直接完成即可)
2. 在usertrap()函数中增加处理因为懒分配而产生的缺页错误。(trap.c)具体操作为，如果遇到缺页错误，那么则检查是否为懒分配，如果是懒分配，就给他分配内存。具体用到一下两个工具函数:
    1. is_lazy_alloc_va(proc.c)
    检查是否为懒分配，在此需要处理两种情况。第一种是虚拟地址大于进程大小，那么一定不是懒分配。第二个是所给的内存在栈的第一页中(这一页不分配，用来处理stack overflow)，懒分配也不应该在这个地方进行。
    2. lazy_alloc(proc.c)
    参考uvmalloc中增长内存的逻辑，在只需要删去dealloc的逻辑即可(因为只分配一次，不用删除之前的分配以恢复原状)
3. 在涉及到缺页恐慌的时候，直接continue即可(默认这是懒分配的缺页，具体在trap中处理)。(uvmcopy, uvmunmap)
4. 现在还有最后一个问题，当系统调用是read或者是write的时候，(以write举例)，调用栈如下:

    1. sys_write
    2. filewrite
    3. pipewrite
    4. copyin
    5. memmove

    其中，copyin的作用为，把从源地址的len个长度拷贝到dst的位置上。这个操作是通过memmove传入两个地址，然后直接通过字节流的方式拷贝的。但是，源地址是用户空间传来的虚拟地址，需要一个转换成物理地址的过程，在此处直接通过walkaddr转换。但是，由于懒分配，我们很可能找不到这样的物理地址。因此，直接修改walkaddr函数，在其中完成懒分配即可。

**实验中遇到的问题和解决方法：**  
在查找copyin和copyout出错原因时，查找了很多内容，最后才明白出错的原因，并考虑修改`walkaddr()`函数考虑懒分配带来的潜在缺页错误，的方法。

**实验心得：**  
通过实现懒分配策略，我学会了如何优化内存使用，并处理内存分配延迟引发的复杂情况。这次实验帮助我理解了内存管理在操作系统中的重要性，以及如何在实际系统中应用这些理论。尽管遇到了缺页处理和系统调用中的挑战，但通过逐步调试和完善代码，我成功实现了目标，掌握了更深层次的内存管理技术。

## lab6: cow

### 阅读笔记

cow: copy on write

对于fork

- 原先逻辑: 将父进程的所有用户空间内存复制到子进程中
- cow: 在fork时，不进行实际的拷贝，只是将虚拟地址指向与父进程相同的物理地址。只有在设计修改操作时，才真正进行拷贝。

### 题目

**实验目的：**
本实验的目的是实现“写时复制”（Copy-On-Write, COW），通过在进程调用`fork`时延迟内存的实际拷贝，来提高内存的利用率和性能。具体来说，父子进程共享相同的物理页面，只有当其中一个进程尝试修改页面时，才进行实际的页面复制操作。

**实验步骤：**

1. 先在PTE的保留位中选一个PTE_COW记录是否为COW的页面。(riscv.h)
2. 新建一个ref数组(kalloc.c)，里面有一个int的cnt数组，记录每个页面的引用计数。同时，这里要处理读写冲突的问题(要加锁)。同时，在kinit()的时候初始化这个锁(kalloc.c)
3. 修改kalloc函数，在分配一页的时候将这一页的引用计数设置为1。(kalloc.c)
4. 修改kfree函数，在释放一页的时候将其引用计数减少。只有引用计数为0的时候，才释放内存。(kalloc.c)
5. 定义了两个工具krefcnt获取一个物理地址的引用计数，kadd_refcnt来增加引用计数。
6. 同样，在usertrap(trap.c)中处理cow引发的错误。需要两个函数is_cow_fault和cow_alloc
    1. is_cow_fault(vm.c)传入pagetable和虚拟地址，先获取页表项，然后进行简单的判断后返回结果(具体涉及PTE_V, PTE_U, PTE_COW)
    2. cow_alloc(vm.c)与上个实验不同的是，这里如果成功，会直接返回分配到的物理地址。具体的分配过程如下: 首先获取对应的物理地址和页表项(在这里不能使用PA2PTE，因为一个物理地址可能对应多个页表项)，然后检查引用计数的状态。如果为1，则将写权限还给他，并不认为这依然是一个COW的页面。如果不为1，则需要分配新的页面，并将旧页面引用计数减去。
7. 修改uvmcopy(vm.c)，在copy的时候，不分配内存，而是共享内存，并收回写权限，设置为COW的页面，并增加引用计数。
8. 修改freerange函数(kalloc.c)，这个函数的目的是把一段物理内存全部释放掉，因此我们直接将其引用计数设置为1，然后调用kfree将其释放即可。
9. 修改copyout()函数，在其中添加cow_fault的逻辑即可。

**实验中遇到的问题和解决方法：**

1. **引用计数管理：**  
   引用计数管理是实验的核心部分，尤其是在并发访问时，需要通过锁机制确保计数的准确性。在实现中使用锁保护`ref`数组，确保在多线程情况下的正确性。

2. **写入用户空间时的COW处理：**  
   `copyout`函数处理用户空间内存写入操作。当涉及到COW页面时，必须进行检查和复制。通过在`copyout`中集成COW分配逻辑，解决了用户空间写入的潜在问题。

**实验心得：**

通过实现COW机制，我进一步理解了操作系统中内存管理的优化技术。COW通过延迟物理页面的实际分配，大幅提高了内存利用率，特别是在`fork`等场景中，显著减少了资源消耗。本实验涉及复杂的内存管理和引用计数逻辑，通过逐步实现和调试，我更好地掌握了这些概念，并成功应用于操作系统开发中。

## lab7: thread

### 阅读笔记

#### 7.1 多路复用

将CPU从一个进程切换到另一个进程实现多路复用。切换情况如下:

1. 等待设备或管道IO
2. 等待子进程退出
3. 调用sleep等待
4. 强制切换长时间计算不睡眠的进程。

目的: 产生每个进程都有自己CPU的错觉

#### 7.2 代码: 上下文切换

在本节中研究内核线程到调度程序线程的切换。

1. 调度程序不能在旧进程的内核栈执行，因为别的内核可能重新唤醒旧进程，从而在两个核心保留了同一个进程的内核栈。因此，每个CPU都有一个专用的调度线程。如图所示:
  ![显示错误](./Resource/Lab7/p1.png)
2. 函数swtch用来执行线程的保存和恢复操作。在此处，上下文将会被切换为保存在cpu中的调度程序的上下文。

#### 7.3 代码: 调度

scheduler选择下一个要进行的进程。

这是一个简单的循环，找到一个可运行的程序，然后将上下文切换过去开始运行。

需要注意的有如下两个不变量:

1. 如果进程是running状态，那么yield必须可以安全地将其切换出去。(没有任何东西有这个东西的副本)
2. 如果进程是runnable状态，那么调度程序必须能安全地运行它。(p->context必须保存寄存器，且没有CPU在这个进程的内核栈上执行)

维护方法: 在一个线程中获取p->lock并在另一个进程中释放。

举例: yield函数将这个状态转换为runnable，因此这时锁被持有，并保持被持有直到scheduler调度完成，此时才释放锁。

#### 7.4 代码: mycpu和myproc

mycpu返回一个指向当前cpu的指针，但是这个返回值很脆弱，因为一旦发生了进程的切换，会导致原先的返回值失效。

myproc返回当前cpu运行的进程的指针，通过禁用中断后从cpu取回进程后启用中断实现。但是这个返回值是安全的，因为即使发生了进程的切换，这个指针也不会变。

#### 总结: 上下文切换流程

![显示错误](./Resource/Lab7/p1.png)

1. 先经过trap,保存用户空间的上下文，进入内核
2. 如果是时间片产生的中断，那么会将当前的cpu资源让渡，调用swtch函数进行上下文的切换，转到scheduler
3. scheduler寻找下一个可以执行的进程，然后通过swtch切换到那个进程去。
4. 返回用户空间，执行用户空间的程序。

![显示错误](./Resource/Lab7/p2.png)

### 题目

#### Uthread: switching between threads

**实验目的：**
为用户级线程系统设计上下文切换机制。

**实验步骤：**

1. 仿照swtch.S的内容，完成uthread_switch.S的内容。
2. 将context的定义拷贝到uthread.c，并在thread的定义中加入一个context
3. 在thread_schedule中调用thread_switch进行上下文的切换
4. 在thread_create中设置ra和sp的值。

**实验心得：**
了解了更多关于上下文切换的内容。

#### using threads

**实验目的：**
修改ph.c中的简单哈希表，使其在多线程使用时也正确。

**实验步骤：**

1. 定义一个lock全局数组，并做好初始化
2. 在put中加锁即可。读不用加锁，因为不改。

#### Barrier

**实验目的：**
实现一个barrier，所有参与的线程在此点必须等待，直到所有其它线程也到达该店

**实验步骤：**

1. 修改barrier函数
2. 这个实现比较简单，如果进程数不相等，就一直等待。如果相等了，就唤醒所有进程，恢复初始状态即可。

**实验心得：**

通过实现锁机制和线程同步的Barrier，我学会了如何在多线程环境中防止竞争条件和确保同步。

## lab8: lock

### 阅读笔记

锁通过一些操作可以强制实现多核的串行。

#### 6.3 代码: 使用锁

xv6的kalloc.c分配器有一个由单个锁保护的空闲列表，同一时间只有一个cpu可以通过这个来分配内存。改进方法为使其拥有多个空闲列表。

#### 6.5 锁和中断处理函数

1. 锁和中断的交互可能会产生死锁。举例: clockintr定时器在处理中断的时候增加ticks，而sys_sleep会读取ticks，因此，加锁tickslock。但是，当sys_sleep持有tickslock时被中断，那么他将在持有锁的时候进入中断处理，并且在中断处理中请求锁tickslock。
2. xv6中，为了解决如上情况，当cpu获取锁的时候，会禁用这个cpu的中断。
3. 通过push_off禁用中断，pop_off恢复中断。同时，这两个函数通过计数的方式考虑了多层锁的情况。

#### 6.9 睡眠锁

由上述内容可知，由于禁用了中断，那么在获取了普通锁的时候不能让渡cpu。但是，有时操作时间过长，会浪费很多。因此，提供了sleep-lock。可以使用acquiresleep在等待时让渡cpu.

#### 8.2 Buffer cache层

bio.c

本层两个任务:

  1. 同步对磁盘块的访问，确保磁盘块在内存中只有一个副本，且只有一个内核访问
  2. 缓存常用块，加快速度

接口：

  1. bread: 获取一个buf,包含一个块的副本
  2. bwrite: 将修改后的缓冲区写回
  3. brelse: 释放缓冲区

这层使用的是睡眠锁，可以中断。使用LRU算法。

#### 8.3 代码: Buffer cache

1. 以双链表表示的缓冲区。使用bcache.head引用链表。
2. bread调用bget为扇区获取缓冲区。如果需要从磁盘读取，那么会调用virtio_disk_rw
3. bget扫描缓冲区列表，如果存在查询的缓冲区，则获取睡眠锁，然后返回这个缓冲区。如果没有缓冲区，那么会创建一个。这个操作先看能不能新建，如果不能，那么会扫描缓冲区列表，找一个引用计数为0的使用。
4. bget通过加锁的方式来确保每个磁盘扇区都最多有一个缓冲区。
5. brelse释放缓冲区，将缓冲区移动到链表的最前面。这样，链表的第一个缓冲区是最近使用的，最后一个缓冲区是最少使用的，bget中可以直接检查后面的缓冲区来进行简单地选择。
6. 通过睡眠锁保护缓冲区的读写。

### 题目

#### Memory allocator

**实验目的：**
本实验的目的是优化xv6中的内存分配器，使其能够更高效地在多核环境下运行。通过为每个CPU分配独立的空闲内存列表，并且在分配和释放内存时减少锁争用，从而提升系统的并发性能和内存分配的效率。

**实验步骤：**

1. 修改kmems,使其每个cpu都有一个。(kalloc.c)
2. 在kinit中初始化所有的锁，并调用freerange来调用kfree将页面加到链表中。(kalloc.c)
3. 修改kfree,关中断，在cpu操作，然后开中断。(kalloc.c)
4. 修改kalloc,如果当前cpu有空闲的，就用当前cpu的，如果没有，就遍历别的cpu,获取它们里面的空闲空间(一页)。(kalloc.c)

**实验中遇到的问题和解决方法：**

最开始的时候没有理解如何进行初始化，通过仔细研读代码发现，只需要实现kfree即可。这样，在最初始的时候，某个(或某几个)cpu通过freerange将内存加入自己的链表中，然后别的cpu调用的时候会从这里获取内存，释放的时候加入到自己的链表中。

**实验心得：**
通过本实验，我学习了如何通过调整内存分配策略来提高系统在多核环境下的性能。为每个CPU分配独立的内存列表，减少了锁的争用，提高了并发性能。这让我更深入理解了内存分配器的设计思路及其在多核系统中的优化策略。同时，通过处理锁与中断的交互问题，掌握了在复杂并发场景下的正确编程技巧。

#### Buffer cache

**实验目的：**
本实验的目的是优化xv6中的缓冲区缓存系统，使其在多核环境下具有更好的性能和并发控制能力。通过实现基于LRU的缓存替换策略，并改进缓冲区的加锁机制，确保系统对磁盘块的访问是同步且高效的。

**实验步骤：**
这里逻辑较为复杂，特别是加锁的逻辑。因此将其单独分出一部分进行说明。

1. 首先修改buf结构体(buf.h)，去除不再需要的prev字段，添加timestamp字段记录最后使用的时间
2. 修改bcache结构体(bio.c)，添加size字段，记录缓存块数量，添加哈希表buckets,添加需要用的一些锁。
3. 在binit函数中初始化所有的锁。在此处，buf作为一个缓冲池使用。由于可以采用数组形式获取，故不需初始化链表。
4. 修改brelse函数，在释放时只需记录时间即可。
5. 修改bpin和bunpin函数，更换为bucket的锁即可。
6. bget函数的修改。在这里分两个部分。
    1. 分配的逻辑:
        1. 根据获取的idx在对应的哈希表中查找，如果找到则添加引用计数后直接返回。
        2. 如果没有找到，那么查看缓冲池是否有未使用的缓冲区。如果有，那么将其加入哈希表的头节点之后。修改参数后直接返回。
        3. 若是已经没有缓冲区了，那么使用LRU算法查找。从当前的bucket开始查找，找到当前bucket中引用计数为0并且timestamp最小的缓存块。如果有的话，就修改参数后返回(这里需要考虑不在同一个bucket的情况)，如果没有的话，就去其它的bucket中查找。因此，这里不是严格的LRU算法。
    2. 加锁的逻辑(bget)
        1. 在步骤1中，只需要遍历或修改一个特定的bucket，只需要对这个bucket加锁即可。
        2. 在步骤2中，由于会对size进行读取和更新，需要获取lock锁。且步骤1和2之间不能释放bucket的锁。因为一旦释放, 则可能有另一个线程对同一缓存块进行访问, 而此时第一个线程可能还正在分配, 缓存块还未更新到bucket链表中, 由于bucket的锁已经释放, 这样第二个线程可以遍历该bucket链表, 同样发现缓存块不存在则去申请分配，从而导致同一块会被多次分配。
        3. 在步骤2结束后，我选择将所有锁释放，将步骤3的分配作为一个独立的过程进行。因为步骤3需要遍历所有的bucket.如果不释放当前的bucket的锁，虽然可以确保同步，但是会占用大量的时间。而如果释放了，则只需要在遍历的时候加一个简单的判断即可统一所有的代码的表现。
        4. 在步骤3中，每次循环到一个bucket,需要对其进行加锁。但是一旦释放了当前bucket的锁，那么在遍历别的bucket的同时可能有另一个线程同样访问该缓存块，导致了重复的分配，因此需要在整个步骤3外面加一个hashlock进行保护。那么现在，情况如下: 有两个线程来获取hashlock锁，一个进入完成了分配，另一个等待分配后重新进入。因此，需要在此处重新检查是否已经有了目标缓存块(这也解决了3中提出的问题)。

**实验中遇到的问题和解决方法：**
在`bget`函数中，锁的使用需要非常谨慎。特别是在跨`bucket`遍历时，必须确保不会产生死锁或资源竞争。通过不断分析，最终找到了死锁的问题。

**实验心得：**
通过本实验，我学习了如何在操作系统中实现和优化一个多核环境下的缓存系统。使用LRU算法和改进的加锁策略，使得缓冲区缓存系统在并发访问时更加高效且安全。在处理锁的同时，我进一步理解了操作系统中锁机制的重要性及其在多线程环境中的应用。这为我以后设计和优化多线程系统打下了坚实的基础。

## lab9: fs

### 阅读笔记

#### 8.4 日志层

xv6通过简单的日志记录来解决文件崩溃问题。

xv6的系统调用不会直接写入磁盘，而是在磁盘的日志中放入所有写入的描述，一旦完成了所有写入操作的记录，就向磁盘提交一条commit记录，此时再将写操作复制到磁盘上。最后会擦除磁盘上的日志。

如果系统发生崩溃并重启，那么文件系统将读取日志。如果日志为完整操作，则回复代码将写操作复制到文件系统中，如果不是完整操作，那么会忽略该日志。

#### 8.5 日志设计

日志由一个header block和一系列logged block组成，header block包含一个数组来记录每个logged block的扇区号，还有一个int来记录logged block的数量。

日志系统可以将多个系统调用的写入累积到一个事物中。

任何单个系统调用都不允许写入超过日志空间的不同块。如果必须超出，需要将写入分解为多个写入。

#### 8.6 代码: 日志

1. begin_op 确保日志系统当前不处于提交状态，并且有足够的日志空间。
2. log_write 将块的扇区号记录，在日志中预定一个位置，并且将缓存固定在block cache中(bpin)
3. end_op 减少未完成系统调用的计数。如果计数为0，那么会通过commit提交事物。

#### 8.7 代码: 块分配器

1. 块分配器在磁盘上维护一个空闲位图，0代表空闲。
2. balloc 分配一个新的磁盘块
3. bfree 释放一个块

#### 8.8 索引节点层

1. 磁盘上的inode都被保存在inode块的连续区域，每个inode大小都相同，因此，可以仅用一个编号来标识和获取一个inode。
2. 磁盘上的inode为struct dinode,type为0表示inode空闲。nlink统计引用此inode的目录条目。addrs记录保存文件内容的磁盘块的块号。
3. 内核会将活动的inode保存在内存中，struct inode是磁盘dinode的副本，ref记录引用内存中这个inode的数量。
4. iget返回的inode在iput调用之前保证有效，并且返回的inode可能没有有用的内容，此时必须调用ilock并从磁盘读取inode.

#### 8.10 代码: Inode包含内容

addrs数组的前NDIRECT个数据块是直接块，接下来的NINDIRECT个数据块称为间接块，在查阅间接块后才能加载真正的数据。

![显示错误](./Resource/Lab9/p2.png)

函数bmap用来分配块(先直接后间接)，itrunc释放文件的块(先直接后间接)

函数stati将inode元数据复制到stat结构体中，并通过stat系统调用向用户公开。

#### 8.12 代码: 路径名

函数namei计算path并返回相应的inode.

#### 8.13 文件描述符层

xv6为每个进程提供了自己的打开文件表或文件描述符。每个打开的文件用一个struct file表示(inode或管道的封装加上一个IO偏移量)。

open打开文件，如果多个进程打开同一相同文件，那么不同的实例将具有不同的IO偏移量。

所有打开的文件都在全局文件表ftable中。

### 题目

#### Large files

**实验目的：**
支持文件系统处理更大的文件，通过添加二级间接块来扩展文件系统的容量。

**实验步骤：**

1. 在fs.h中更改宏定义，将直接块改为11，添加NDINDIRECT，记录二级间接块所包含的块数。
2. 由于NDIRECT的改变，修改所有相关的数组定义，确保大小不变
3. 修改bmap支持二级索引，在二级索引中，先获取第一级索引的id,再获取第二级索引的id,然后分层地进入去读取相应的内容。
4. 修改itrunc释放所有块，释放二级索引时，需要先释放第二级里面的内容，然后释放第一级对应的内容，防止内容丢失。

**实验中遇到的问题和解决方法：**
在释放块时，特别注意先释放内层索引，再释放外层索引，以避免内存泄漏或数据丢失。

**实验心得：**
通过实现二级间接块扩展了文件系统的容量，但也增加了块管理的复杂度。这需要更严谨的内存和索引管理，以确保系统的稳定性。

#### Symbolic links

**实验目的：**
实现符号链接（symlink）功能，使文件系统支持符号链接的创建与解析(类似超链接)。

**实验步骤：**

1. 首先定义symlink的系统调用
2. 根据指导书要求，去stat.h和fcntl.h中添加两个新的标志指示链接类型。
3. 实现sys_symlink函数(sysfile.c)，分配一个inode节点，向这个节点中写入真正的路径。
4. 修改sys_open系统调用，避免循环的方式是设定一个阈值10。先通过readi读取符号链接指向的地址，然后再通过namei读取新的地址对应的ip,一直循环，直到找到真正的文件。

**实验中遇到的问题和解决方法：**
如果通过递归函数方式实现符号链接指向符号链接，效率过低。因此使用了通过循环方式实现。

**实验心得：**
实现符号链接增强了文件系统的灵活性，使得文件访问更加动态。同时，处理符号链接的循环和递归问题，提高了代码的健壮性。

## lab10: mmap

### 题目

**实验目的：**
本实验的目的是实现内存映射机制（mmap），以支持将文件内容映射到进程的虚拟地址空间中。通过 mmap 和 munmap 系统调用，允许进程高效地访问文件数据。

**实验步骤：**

1. 先添加mmap和munmap的系统调用。
2. 在proc.h中定义NVMA以及vm_area结构体(用于表示mmap系统调用文件映射的虚拟内存的位置、大小、权限等)，并在struct proc中添加相关字段。每个进程都使用一个vma来记录映射的内存。
3. 在allocproc中将vma初始化为0。(proc.c)
4. 在sysfile.c中实现系统调用sys_mmap，先读取参数，然后根据题意做出相应的检查，如果检查通过，则寻找一个未使用的vma,将其赋值，然后添加引用计数后进行懒分配，返回其地址。
5. 在usertrap中处理懒分配的页面错误。如果出现错误的虚拟地址在合理的范围，那么就调用mmap_handler进行处理。首先找到对应的vma，然后检查权限错误，然后分配内存，将文件内容写入后，映射到页表中。(trap.c)
6. 实现系统调用sys_munmap,首先读取参数，然后找到要取消映射的vma。如果需要写回，就用filewrite写回，然后取消页表的映射。如果这个vma的所有映射都被取消，那么就将文件关闭(引用计数-1)。
7. 在uvmunmap和uvmcopy中，取消由懒分配导致的panic。(vm.c)
8. 修改exit,将已经映射的空间取消映射(proc.c)
9. 修改fork，复制父进程的vma(proc.c)

**实验中遇到的问题和解决方法：**

1. **页面错误处理**：实现 mmap 时，最复杂的部分是处理懒分配的页面错误。需要确保页面错误处理程序能够正确处理文件映射的区域，包括权限检查和内存分配。
2. **多线程并发问题**：确保多个线程调用 mmap 或 munmap 时，能够正确处理 vma 的分配和释放，避免 race condition。

**实验心得：**
通过本次实验，我深入理解了 mmap 的实现原理及其在操作系统中的作用。懒分配和页面错误处理机制的实现，进一步加深了我对内存管理的理解。实验的难点在于对并发场景的处理，特别是在处理多个线程同时调用 mmap/munmap 时的正确性。

## lab11: net

### 阅读笔记

这个lab需要读的内容太多了，暂时没有全部看完，只看了一些网上的博客，然后根据博客的内容复现出这个实验并通过了测试。

<https://blog.csdn.net/LostUnravel/article/details/121437373>

总结:

操作系统想要发送数据的时候，将数据放入环形缓冲区数组 tx_ring 内，然后递增 E1000_TDT，网卡会自动将数据发出。当网卡收到数据的时候，网卡首先使用 direct memory access，将数据放入 rx_ring 环形缓冲区数组中，然后向 CPU 发起一个硬件中断，CPU 在收到中断后，直接读取 rx_ring 中的数据即可。

### 题目

**实验目的：**
实现NIC的设备驱动程序

**实验步骤：**

1. e1000_transmit
    1. 首先需要获取锁，避免多进程同时发送数据出现冲突
    2. 获取下一个可用的buffer下标tail
    3. 如果buffer的数据没传输完，意味着缓冲区不足，返回错误即可。
    4. 设置参数
    5. 环形缓冲区下标+1
2. e1000_recv
    1. 一次recv可能接受很多包，因此采用循环。
    2. 调用net_rx传递给上层网络栈，上层释放mbuf
    3. 分配新的mbuf,供下一次使用。

**实验心得：**
通过这个实验，我深入了解了操作系统中的网络栈和驱动程序的实现。尤其是在 e1000 驱动的实现过程中，环形缓冲区的管理和中断处理是关键的难点。这次实验使我认识到网络编程在操作系统中的重要性及其复杂性。
